{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных для LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем нужные нам темы и необходимое число статей для каждой темы. Словарь dataOption в качестве ключей хранит название темы, а в качестве значений число статей для соответствующей темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optionsLDA(object):\n",
    "    def __init__(self, path, topics, nWordPerTopic=100):\n",
    "        self.alpha = 0.1\n",
    "        self.beta = 0.1\n",
    "        \n",
    "        self.topics = topics\n",
    "        # сформировать словари wordid, docid, docs\n",
    "        self.idToDoc = {}\n",
    "        self.wordToId = {}\n",
    "        self.docs = []\n",
    "        last_wordid = -1\n",
    "        last_docid = -1\n",
    "        \n",
    "        \n",
    "        self.nArticles = []\n",
    "        for topicName in topics:\n",
    "            list_articles = os.listdir(path + topicName)\n",
    "            list_articles = np.random.permutation(list_articles)\n",
    "            \n",
    "            nWords = 0\n",
    "            nArticles = 0\n",
    "            \n",
    "            for article in list_articles: \n",
    "                # Создадим id для статьи\n",
    "                last_docid += 1\n",
    "                self.idToDoc[last_docid] = topicName + '/' + article\n",
    "\n",
    "                nArticles += 1\n",
    "                # Обработка статьи\n",
    "                doc = []\n",
    "                with codecs.open(path + topicName + '/' + article, 'r', 'utf8') as inf:\n",
    "                    for line in inf:\n",
    "                        line = line.split()\n",
    "                        nWords += len(line)\n",
    "                        for w in line:\n",
    "                            if w not in self.wordToId.keys():\n",
    "                                last_wordid += 1\n",
    "                                self.wordToId[w] = last_wordid\n",
    "                            doc.append(self.wordToId[w])\n",
    "                self.docs.append(doc)\n",
    "                if nWords > nWordPerTopic:\n",
    "                    break  \n",
    "            self.nArticles.append(nArticles)\n",
    "        self.M = len(self.docs)\n",
    "        self.V = len(self.wordToId.keys())\n",
    "    def info(self):\n",
    "        print \"Количество статей в каждой категории: \"\n",
    "        for t,a in zip(self.topics, self.nArticles):\n",
    "            print u'[%s:%d] '%(t,a),\n",
    "            \n",
    "        print \"\\nРазмер словаря: %d\"%(self.V)\n",
    "        print \"Количество статей: %d\"%(self.M)\n",
    "        print \"\\nТОР 20 самых часто встречающихся слов из каждой темы:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Власть] [Технологии] [Происшествия] [Строительство] [Деньги] [Работа] [Недвижимость] [Туризм] [ЖКХ] [Общество] [Авто] [Финансы] [Спорт] [Город] [Бизнес] [Доброе дело]\n"
     ]
    }
   ],
   "source": [
    "path = \"./preprocessedNews/\"\n",
    "for d in os.listdir(path):\n",
    "    print \"[%s]\"%d, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options = optionsLDA(path, [u'Авто', u'Спорт', u'Доброе дело'], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество статей в каждой категории: \n",
      "[Авто:96]  [Спорт:114]  [Доброе дело:55]  \n",
      "Размер словаря: 7683\n",
      "Количество статей: 265\n",
      "\n",
      "ТОР 20 самых часто встречающихся слов из каждой темы:\n"
     ]
    }
   ],
   "source": [
    "options.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "## Реализация класса LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " <img src=smoothed_lda.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    def __init__(self, K, option):\n",
    "        self.K = K\n",
    "        self.alpha = option.alpha\n",
    "        self.beta = option.beta\n",
    "        \n",
    "        self.Voc = {}\n",
    "        for k,v in option.wordToId.items():\n",
    "            self.Voc[v] = k\n",
    "        self.V = option.V\n",
    "        \n",
    "        \n",
    "        self.docs = options.docs\n",
    "        self.M = option.M\n",
    "    \n",
    "        \n",
    "        self.z_mn = []                                 # тема n-го слова в документе m\n",
    "        self.n_mz = np.zeros((self.M, self.K)) + self.alpha # число слов в документе m с темой z\n",
    "        self.n_zt = np.zeros((self.K, self.V)) + self.beta  # сколько раз слово t встречается в теме z\n",
    "        self.n_z  = np.zeros((self.K)) + self.V*self.beta   # число слов в каждой теме\n",
    "        \n",
    "        # смотрим все документы\n",
    "        for m, doc in enumerate(self.docs):\n",
    "            # для каждого слова выбираем тему\n",
    "            z_n = []                           # z_n[i] хранит тему для i-го слова\n",
    "            \n",
    "            # смотрим все слова в этом документе, wordid - ид слова, nw - сколько раз слово встречается в документе\n",
    "            for t in doc:                \n",
    "                # выбираем тему\n",
    "                #z = np.random.randint(0, self.K)\n",
    "                p_z = self.n_zt[:,t] * self.n_mz[m] / self.n_z\n",
    "                z = np.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "                z_n.append(z)\n",
    "\n",
    "                # обновляем матрицы\n",
    "                self.n_mz[m, z]      += 1.0\n",
    "                self.n_zt[z, t]      += 1.0\n",
    "                self.n_z[z]          += 1.0\n",
    "            self.z_mn.append(np.array(z_n))\n",
    "            \n",
    "    def iteration(self):\n",
    "        \"\"\"Одна итерация\"\"\"\n",
    "        # смотрим все документы\n",
    "        for m,doc in enumerate(self.docs):\n",
    "            for n, t in enumerate(doc):\n",
    "                # выбираем тему\n",
    "                z = self.z_mn[m][n]\n",
    "                #print self.n_mz[docid, z], self.n_zt[z, wordid], self.n_z[z]\n",
    "                self.n_mz[m,z]   -= 1.0\n",
    "                self.n_zt[z, t] -= 1.0\n",
    "                self.n_z[z]          -= 1.0\n",
    "                #print self.n_mz[docid, z], self.n_zt[z, wordid], self.n_z[z]\n",
    "\n",
    "                #выбриаем новую тему\n",
    "                p_z = (self.n_zt[:,m]) * (self.n_mz[m]) / (self.n_z)\n",
    "\n",
    "                #print p_z\n",
    "                #print p_z.sum()\n",
    "                new_z = np.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "                self.z_mn[m][n] = new_z\n",
    "                self.n_mz[m, new_z]      += 1.0\n",
    "                self.n_zt[new_z, t]      += 1.0\n",
    "                self.n_z[new_z]          += 1.0\n",
    "\n",
    "    def get_phi(self):\n",
    "        return self.n_zt / self.n_z[:, np.newaxis]\n",
    "    \n",
    "    def perplexity(self):\n",
    "        phi = self.get_phi()\n",
    "        N = 0\n",
    "        logPerplexity = 0\n",
    "        \n",
    "        for m,doc in enumerate(self.docs):\n",
    "            theta = self.n_mz[m] /(len(doc) + self.K * self.alpha)\n",
    "            for w in doc:\n",
    "                logPerplexity -= np.log(np.inner(phi[:,w], theta))\n",
    "            N += len(doc)\n",
    "        return np.exp(logPerplexity / N)\n",
    "    \n",
    "    \n",
    "    def learn(self, n_iter):\n",
    "        last_perplexity = self.perplexity()\n",
    "        print \"Начальное значение perplexity: %f\" % last_perplexity\n",
    "        for i in range(n_iter):\n",
    "            self.iteration()\n",
    "            perplexity = self.perplexity()\n",
    "            print \"Итерация %d: perplexity=%f\"%(i+1,perplexity)\n",
    "            #if last_perplexity < perplexity:\n",
    "            #    print \"model is learned\"\n",
    "            #    return\n",
    "            if perplexity - last_perplexity > 20:\n",
    "                print \"OK\"\n",
    "                return\n",
    "            if last_perplexity > perplexity:\n",
    "                perplexity = last_perplexity\n",
    "        \n",
    "    def get_wt_distribution(self):\n",
    "        phi = self.get_phi()\n",
    "        for i in range(self.K):\n",
    "            print \"\\n\\ntopic %d\"%i\n",
    "            for w in np.argsort(-phi[i])[:20]:\n",
    "                print \"+ %f*%s\"%(phi[i,w], self.Voc[w]),\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylda = LDA(3, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальное значение perplexity: 2774.938378\n",
      "Итерация 1: perplexity=2780.938070\n",
      "Итерация 2: perplexity=2747.995510\n",
      "Итерация 3: perplexity=2727.967620\n",
      "Итерация 4: perplexity=2737.467532\n",
      "Итерация 5: perplexity=2732.245950\n",
      "Итерация 6: perplexity=2712.264593\n",
      "Итерация 7: perplexity=2707.046242\n",
      "Итерация 8: perplexity=2695.743503\n",
      "Итерация 9: perplexity=2674.410200\n",
      "Итерация 10: perplexity=2663.553543\n"
     ]
    }
   ],
   "source": [
    "mylda.learn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Авто] [Спорт] [Доброе дело] \n",
      "\n",
      "topic 0\n",
      "+ 0.006179*россия + 0.005732*команда + 0.004749*матч + 0.004033*чемпионат + 0.003944*ребёнок + 0.003854*стать + 0.003765*зенит + 0.003675*петербург + 0.003497*клуб + 0.003139*автомобиль + 0.002960*фонд + 0.002871*игра + 0.002692*рубль + 0.002334*участие + 0.002334*ска + 0.002334*напомнить + 0.002245*сборный + 0.002155*российский + 0.002155*проект + 0.002155*тренер \n",
      "\n",
      "topic 1\n",
      "+ 0.005376*ребёнок + 0.005150*петербург + 0.004698*рубль + 0.004189*фонд + 0.004019*россия + 0.003963*дом + 0.003567*благотворительный + 0.003454*район + 0.003115*проект + 0.002719*место + 0.002663*стать + 0.002380*организация + 0.002380*сообщать + 0.002323*автомобиль + 0.002267*улица + 0.002154*социальный + 0.002154*зенит + 0.002097*матч + 0.002097*система + 0.002041*чемпионат \n",
      "\n",
      "topic 2\n",
      "+ 0.005374*россия + 0.005374*петербург + 0.004811*автомобиль + 0.004248*транспортный + 0.003686*рубль + 0.003686*матч + 0.003404*чемпионат + 0.003404*место + 0.002560*средство + 0.002560*организация + 0.002560*пройти + 0.002560*официальный + 0.002279*зенит + 0.002279*помощь + 0.002279*состав + 0.002279*образ + 0.002279*налог + 0.002279*движение + 0.001998*петербургский + 0.001998*напомнить\n"
     ]
    }
   ],
   "source": [
    "for t in options.topics:\n",
    "    print \"[%s]\"%t,\n",
    "mylda.get_wt_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
